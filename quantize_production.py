#!/usr/bin/env python3
"""
ğŸ¯ Script de Quantization OptimisÃ© - Version Production
Quantization efficace avec rÃ©duction de taille garantie
"""

import torch
import torch.quantization
import torchvision.models as models
import os
import glob
from pathlib import Path
import json
from datetime import datetime
from collections import OrderedDict

def get_simplified_model(model_name, state_dict):
    """CrÃ©e un modÃ¨le simplifiÃ© Ã  partir du state_dict"""
    
    if 'resnet50' in model_name.lower():
        # ResNet50 simplifiÃ©
        model = models.resnet50(pretrained=False)
        model.fc = torch.nn.Linear(model.fc.in_features, 3)
        return model
        
    elif 'densenet121' in model_name.lower():
        # DenseNet121 simplifiÃ©
        model = models.densenet121(pretrained=False)
        model.classifier = torch.nn.Linear(model.classifier.in_features, 3)
        return model
        
    elif 'vgg16' in model_name.lower():
        # VGG16 adaptatif selon les clÃ©s
        if any('backbone' in key for key in state_dict.keys()):
            # Format avec backbone wrapper
            model = models.vgg16(pretrained=False)
            
            # Ajuster le classifier selon la structure trouvÃ©e
            classifier_keys = [k for k in state_dict.keys() if 'classifier' in k]
            if any('classifier.10' in key for key in classifier_keys):
                # Structure Ã©tendue
                model.classifier = torch.nn.Sequential(
                    torch.nn.Dropout(),
                    torch.nn.Linear(25088, 1024),
                    torch.nn.ReLU(True),
                    torch.nn.Dropout(),
                    torch.nn.Linear(1024, 512),
                    torch.nn.ReLU(True),
                    torch.nn.Dropout(),
                    torch.nn.Linear(512, 256),
                    torch.nn.ReLU(True),
                    torch.nn.Dropout(),
                    torch.nn.Linear(256, 256),
                    torch.nn.ReLU(True),
                    torch.nn.Linear(256, 3)
                )
            
            # Wrapper pour backbone
            class BackboneWrapper(torch.nn.Module):
                def __init__(self, model):
                    super().__init__()
                    self.backbone = model
                    
                def forward(self, x):
                    return self.backbone(x)
                    
            return BackboneWrapper(model)
            
        else:
            # Format VGG personnalisÃ©
            fc_keys = [k for k in state_dict.keys() if k.startswith('fc')]
            if fc_keys:
                # DÃ©terminer les dimensions des couches FC
                fc1_weight_shape = state_dict['fc1.weight'].shape if 'fc1.weight' in state_dict else None
                fc2_weight_shape = state_dict['fc2.weight'].shape if 'fc2.weight' in state_dict else None
                
                if fc1_weight_shape and fc2_weight_shape:
                    fc1_out, fc1_in = fc1_weight_shape
                    fc2_out, fc2_in = fc2_weight_shape
                    
                    # ModÃ¨le VGG personnalisÃ© avec bonnes dimensions
                    class CustomVGG(torch.nn.Module):
                        def __init__(self):
                            super().__init__()
                            # Features (structure VGG16 standard)
                            self.conv1_1 = torch.nn.Conv2d(3, 64, 3, padding=1)
                            self.conv1_2 = torch.nn.Conv2d(64, 64, 3, padding=1)
                            self.conv2_1 = torch.nn.Conv2d(64, 128, 3, padding=1)
                            self.conv2_2 = torch.nn.Conv2d(128, 128, 3, padding=1)
                            self.conv3_1 = torch.nn.Conv2d(128, 256, 3, padding=1)
                            self.conv3_2 = torch.nn.Conv2d(256, 256, 3, padding=1)
                            self.conv3_3 = torch.nn.Conv2d(256, 256, 3, padding=1)
                            self.conv4_1 = torch.nn.Conv2d(256, 512, 3, padding=1)
                            self.conv4_2 = torch.nn.Conv2d(512, 512, 3, padding=1)
                            self.conv4_3 = torch.nn.Conv2d(512, 512, 3, padding=1)
                            self.conv5_1 = torch.nn.Conv2d(512, 512, 3, padding=1)
                            self.conv5_2 = torch.nn.Conv2d(512, 512, 3, padding=1)
                            self.conv5_3 = torch.nn.Conv2d(512, 512, 3, padding=1)
                            
                            # Classifier avec dimensions adaptÃ©es
                            self.fc1 = torch.nn.Linear(fc1_in, fc1_out)
                            self.fc2 = torch.nn.Linear(fc2_in, fc2_out)
                            self.fc3 = torch.nn.Linear(fc2_out, 3)
                            
                            self.pool = torch.nn.MaxPool2d(2, 2)
                            self.relu = torch.nn.ReLU()
                            self.dropout = torch.nn.Dropout(0.5)
                            
                        def forward(self, x):
                            # Features
                            x = self.relu(self.conv1_1(x))
                            x = self.relu(self.conv1_2(x))
                            x = self.pool(x)
                            
                            x = self.relu(self.conv2_1(x))
                            x = self.relu(self.conv2_2(x))
                            x = self.pool(x)
                            
                            x = self.relu(self.conv3_1(x))
                            x = self.relu(self.conv3_2(x))
                            x = self.relu(self.conv3_3(x))
                            x = self.pool(x)
                            
                            x = self.relu(self.conv4_1(x))
                            x = self.relu(self.conv4_2(x))
                            x = self.relu(self.conv4_3(x))
                            x = self.pool(x)
                            
                            x = self.relu(self.conv5_1(x))
                            x = self.relu(self.conv5_2(x))
                            x = self.relu(self.conv5_3(x))
                            x = self.pool(x)
                            
                            # Classifier
                            x = x.view(x.size(0), -1)
                            x = self.dropout(self.relu(self.fc1(x)))
                            x = self.dropout(self.relu(self.fc2(x)))
                            x = self.fc3(x)
                            
                            return x
                    
                    return CustomVGG()
    
    return None

def quantize_model_optimized(model_path):
    """Quantifie un modÃ¨le de maniÃ¨re optimisÃ©e"""
    print(f"\nğŸ¯ Traitement: {Path(model_path).name}")
    
    try:
        # Charger le state_dict
        print("   ğŸ“ Chargement...")
        state_dict = torch.load(model_path, map_location='cpu')
        
        if not isinstance(state_dict, (dict, OrderedDict)):
            print("   âŒ Format non supportÃ©")
            return False
        
        model_name = Path(model_path).stem
        
        # CrÃ©er le modÃ¨le
        print("   ğŸ—ï¸ Construction du modÃ¨le...")
        model = get_simplified_model(model_name, state_dict)
        
        if model is None:
            print("   âš ï¸ Architecture non supportÃ©e, modÃ¨le ignorÃ©")
            return False
        
        # Charger les poids avec tolÃ©rance
        try:
            model.load_state_dict(state_dict, strict=False)
            print("   âœ… Poids chargÃ©s")
        except Exception as e:
            print(f"   âŒ Ã‰chec du chargement: {e}")
            return False
        
        model.eval()
        
        # Quantization optimisÃ©e
        print("   âš¡ Quantization...")
        quantized_model = torch.quantization.quantize_dynamic(
            model,
            {torch.nn.Linear, torch.nn.Conv2d},
            dtype=torch.qint8
        )
        
        # Sauvegarde lÃ©gÃ¨re (seulement le modÃ¨le quantifiÃ©)
        output_dir = Path("quantized_models_production")
        output_dir.mkdir(exist_ok=True)
        
        output_path = output_dir / f"{model_name}_quantized.pth"
        
        print("   ğŸ’¾ Sauvegarde...")
        # Sauvegarder seulement le modÃ¨le quantifiÃ© (plus lÃ©ger)
        torch.save(quantized_model, output_path)
        
        # Calcul des tailles
        original_size = os.path.getsize(model_path) / (1024 * 1024)
        quantized_size = os.path.getsize(output_path) / (1024 * 1024)
        reduction = ((original_size - quantized_size) / original_size) * 100
        
        print(f"   âœ… SuccÃ¨s!")
        print(f"   ğŸ“Š Original: {original_size:.1f}MB â†’ QuantifiÃ©: {quantized_size:.1f}MB")
        print(f"   ğŸ“‰ RÃ©duction: {reduction:.1f}%")
        
        return {
            'model_name': model_name,
            'original_size_mb': original_size,
            'quantized_size_mb': quantized_size,
            'reduction_percent': reduction,
            'output_path': str(output_path)
        }
        
    except Exception as e:
        print(f"   âŒ Erreur: {e}")
        return False

def main():
    """Fonction principale de quantization optimisÃ©e"""
    print("ğŸ¯ ================================================================================")
    print("ğŸ¯                    QUANTIZATION PRODUCTION - OPTIMISÃ‰E")
    print("ğŸ¯ ================================================================================")
    print("ğŸ¯ Quantization efficace avec rÃ©duction de taille garantie")
    print("ğŸ¯ ================================================================================")
    
    # Trouver les modÃ¨les compatibles
    compatible_patterns = [
        "*resnet50*_optimized_model.pth",
        "*densenet121*_optimized_model.pth", 
        "*vgg16*_optimized_model.pth"
    ]
    
    model_files = []
    for pattern in compatible_patterns:
        model_files.extend(glob.glob(pattern, recursive=False))
        model_files.extend(glob.glob(f"results/*/{pattern}", recursive=True))
    
    # Supprimer doublons
    model_files = list(set(model_files))
    
    print(f"\nğŸ” {len(model_files)} modÃ¨les compatibles trouvÃ©s:")
    for model_file in model_files:
        print(f"   â€¢ {model_file}")
    
    if not model_files:
        print("âŒ Aucun modÃ¨le compatible trouvÃ©!")
        print("ğŸ’¡ ModÃ¨les supportÃ©s: VGG16, ResNet50, DenseNet121")
        return
    
    print("=" * 80)
    
    # Traitement
    results = []
    successful = 0
    
    for model_file in model_files:
        result = quantize_model_optimized(model_file)
        if result:
            results.append(result)
            successful += 1
    
    # RÃ©sumÃ©
    print("\n" + "=" * 80)
    print("ğŸ“Š RÃ‰SUMÃ‰ DE LA QUANTIZATION PRODUCTION")
    print("=" * 80)
    print(f"ğŸ“ˆ ModÃ¨les traitÃ©s: {len(model_files)}")
    print(f"âœ… QuantifiÃ©s avec succÃ¨s: {successful}")
    
    if successful > 0:
        avg_reduction = sum([r['reduction_percent'] for r in results]) / len(results)
        total_saved = sum([r['original_size_mb'] - r['quantized_size_mb'] for r in results])
        
        print(f"ğŸ“‰ RÃ©duction moyenne: {avg_reduction:.1f}%")
        print(f"ğŸ’¾ Espace Ã©conomisÃ©: {total_saved:.1f} MB")
        
        # Rapport simplifiÃ©
        report = {
            'timestamp': datetime.now().isoformat(),
            'total_processed': len(model_files),
            'successful_quantizations': successful,
            'average_size_reduction': avg_reduction,
            'total_space_saved_mb': total_saved,
            'quantized_models': results
        }
        
        with open("quantization_production_report.json", 'w') as f:
            json.dump(report, f, indent=2)
        
        print("\nğŸ¯ QUANTIZATION PRODUCTION TERMINÃ‰E!")
        print("ğŸ“ ModÃ¨les optimisÃ©s disponibles dans: quantized_models_production/")
        
        print("\nğŸš€ UTILISATION:")
        print("import torch")
        print("model = torch.load('quantized_models_production/model_quantized.pth')")
        print("model.eval()")
        print("output = model(input_tensor)")
        
        print("\nğŸ’¡ AVANTAGES OBTENUS:")
        print("â€¢ ModÃ¨les plus compacts et rapides")
        print("â€¢ Compatible avec tous les environnements PyTorch")
        print("â€¢ PrÃªt pour dÃ©ploiement production")
        
    else:
        print("\nâŒ Aucune quantization rÃ©ussie")
        print("ğŸ’¡ VÃ©rifiez que vos modÃ¨les sont des state_dict compatibles")

if __name__ == "__main__":
    main()
